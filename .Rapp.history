save.pkg.list <- installed.packages()[is.na(installed.packages()[ , "Priority"]), 1]
save(save.pkg.list, file="pkglist.Rdata")
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list
typeof(save.pkg.list)
x<-x[-which(x==4)]
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="zoo")]
save.pkg.list
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
save.pkg.list
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
install.packages(save.pkg.list)
Sys.getenv("PATH")
load("pkglist.Rdata")
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
install.packages(save.pkg.list)
library(party)#
library(rpart)#
library(ggplot2)#
library(randomForest)
head(kyphosis)
summary(kyphosis)
length(kyphosis)
require(devtools)
install_github("slidify", "ramnathv")
library(AER)#
library(corrplot)#
library(corrgram)#
library(caret)#
library(randomForest)#
library(klaR)#
library(pROC)#
#
# Replace the path here with the appropriate one for your machine#
myprojectpath = "/Users/eoinbrazil/Desktop/DublinR/TreesAndForests/DublinR-ML-treesandforests/"#
#
# Set the working directory to the current location for the project files#
setwd(myprojectpath)#
#
# Replace the path here with the appropriate one for your machine#
scriptpath = paste(myprojectpath, "scripts/", sep="")#
datapath = paste(myprojectpath, "data/", sep="")#
graphpath = paste(myprojectpath, "graphs/", sep="")#
# Set the seed so comparisons can be later made between the methods#
set.seed(2323)#
#
# Load Affairs data#
data(Affairs)#
#
# Explore the first ten records of the dataset to take a peek#
head(Affairs)#
#
# Look to get a summary of the dataset for each variable in the dataframe#
summary(Affairs)#
#
# Add some additional fields to data to for the correlation analysis#
Affairs$male <- mapvalues(Affairs$gender, from = c("female", "male"), to = c(0, 1))#
Affairs$male <- as.integer(Affairs$male)#
Affairs$male <- sapply(Affairs$male, function(x) x-1)#
Affairs$kids <- mapvalues(Affairs$children, from = c("no", "yes"), to = c(0, 1))#
Affairs$kids <- as.integer(Affairs$kids)#
Affairs$kids <- sapply(Affairs$kids, function(x) x-1)#
#
# Add a field to indicate if there was any affairs#
Affairs$hadaffair[Affairs$affairs< 1] <- 'No'#
Affairs$hadaffair[Affairs$affairs>=1] <- 'Yes'#
Affairs$hadaffair <- as.factor(Affairs$hadaffair)#
table(Affairs$hadaffair)#
#
# Check that the fields are only numerical in the data frame#
sapply(Affairs[, -c(2,5,12)], is.numeric)#
#
# Explore the data set to identify and then we'll remove any moderately correlated variables (could change the correlation value from 0.65 to .9)#
affairs.corr <- cor(Affairs[, -c(2,5,12])#
corrplot(affairs.corr, method = "number", tl.cex = 0.6)#
affairs.variables.corr <- findCorrelation(affairs.corr, 0.65)#
affairs.variables.corr.names <- colnames(affairs.corr[,affairs.variables.corr])#
#
# The dataset requires defined training and test subsets so let's remove some of the variables that don't see to add to the value and create these#
# in processing unwanted we also include the variables quality and color (13,15) as we want to remove these as well#
trainIndices = createDataPartition(Affairs$hadaffair, p = 0.8, list = F)#
#
unwanted = colnames(Affairs) %in% c("yearsmarried", "affairs")#
affairs.df.train = Affairs[trainIndices, !unwanted] #remove affairs and yearsmarried#
affairs.df.test = Affairs[!1:nrow(Affairs) %in% trainIndices, !unwanted]#
table(affairs.df.test$hadaffair)#
#
cv.opts = trainControl(method="cv", number=10, classProbs = TRUE)#
rf.opts = data.frame(.mtry=c(2:6))#
results.rf = train(hadaffair~., data=affairs.df.train, method="rf", preProcess='range',trControl=cv.opts, tuneGrid=rf.opts, n.tree=1000)#
results.rf#
#
# Plot the#
plot(results.rf, scales = list(x = list(log = 10)))#
#
# Look at the results of the random forest based on the testing dataset for predicting new samples#
rfPred <- predict(results.rf, affairs.df.test[,-10])#
confusionMatrix(rfPred, affairs.df.test$hadaffair, positive="Yes")#
#
# Explore the random forest using the class probabilities for different models and an ROC curve for predicting new samples#
rfPredProb <- predict(results.rf, affairs.df.test[,-10], type='prob')#
rfROC <- roc(affairs.df.test$hadaffair, rfPredProb[, "Yes"], levels = rev(affairs.df.test$hadaffair))#
plot(rfROC, type = "S", print.thres = .5)#
#
cv.opts = trainControl(method="cv", number=10, classProbs = TRUE)#
results.nb = train(hadaffair~., data=affairs.df.train, method="nb", preProcess='range',trControl=cv.opts)#
results.nb #
#
# Plot the#
plot(results.nb)#
#
# Look at the results of the naive bayes based on the testing dataset for predicting new samples#
nbPred <- predict(results.nb, affairs.df.test[,-10])#
confusionMatrix(nbPred, affairs.df.test$hadaffair, positive="Yes")#
#
# Explore the random forest using the class probabilities for different models and an ROC curve for predicting new samples#
nbPredProb <- predict(results.nb, affairs.df.test[,-10], type='prob')#
nbROC <- roc(affairs.df.test$hadaffair, nbPredProb[, "Yes"], levels = rev(affairs.df.test$hadaffair))#
plot(nbROC, type = "S", print.thres = .5)
svg(filename = "Standard deviation diagram.svg", width = 7, height = 3.5)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
# library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models" type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
svg(filename = "Standard deviation diagram.svg", width = 7, height = 3.5)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
# library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models" type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
# library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models" type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models" type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models" type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, main="Assessing Classifier Models", type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))
svg(filename = "Standard deviation diagram.svg", width = 7, height = 3.5)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
png(filename = "Standard deviation diagram.png", width = 7, height = 3.5)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
png(filename = "Standard deviation diagram.png", width = 7, height = 5)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
png(filename = "Standard deviation diagram.png", width = 3, height = 3)#
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
# Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))#
dev.off()
par(mar = c(2, 2, 0, 0))#
# # External package to generate four shades of blue#
##
 library(RColorBrewer)#
# cols <- rev(brewer.pal(4, "Blues"))#
cols <- c("#2171B5", "#6BAED6", "#BDD7E7", "#EFF3FF")#
#
# Sequence between -4 and 4 with 0.1 steps#
x <- seq(-4, 4, 0.1)#
#
# Plot an empty chart with tight axis boundaries, and axis lines on bottom and left#
plot(x, type="n", xaxs="i", yaxs="i", xlim=c(-4, 4), ylim=c(0, 0.4),#
     bty="l", xaxt="n", xlab="", ylab="")#
#
# Function to plot each coloured portion of the curve, between "a" and "b" as a#
# polygon; the function "dnorm" is the normal probability density function#
polysection <- function(a, b, col, n=11){#
    dx <- seq(a, b, length.out=n)#
    polygon(c(a, dx, b), c(0, dnorm(dx), 0), col=col, border=NA)#
    # draw a white vertical line on "inside" side to separate each section#
    segments(a, 0, a, dnorm(a), col="white")#
}#
#
# Build the four left and right portions of this bell curve#
for(i in 0:3){#
    polysection(   i, i+1,  col=cols[i+1]) # Right side of 0#
    polysection(-i-1,  -i,  col=cols[i+1]) # Left right of 0#
}#
#
# Black outline of bell curve#
lines(x, dnorm(x))#
#
# Bottom axis values, where sigma represents standard deviation and mu is the mean#
axis(1, at=-3:3, labels=expression(-3*sigma, -2*sigma, -1*sigma, mu,#
                                    1*sigma,  2*sigma,  3*sigma))#
#
#png(filename = "Standard deviation diagram.png", width = 3, height = 3)#
 Add percent densities to each division (rounded to 1 decimal place), between x and x+1#
pd <- sprintf("%.1f%%", 100*(pnorm(1:4) - pnorm(0:3)))#
text(c((0:3)+0.5,(0:-3)-0.5), c(0.16, 0.05, 0.04, 0.02), pd, col=c("white","white","black","black"))#
segments(c(-2.5, -3.5, 2.5, 3.5), dnorm(c(2.5, 3.5)), c(-2.5, -3.5, 2.5, 3.5), c(0.03, 0.01))
library(RRF); # install RRF package before running the script
library(AER)#
library(corrplot)#
library(corrgram)#
library(caret)#
library(randomForest)#
library(klaR)#
library(pROC)#
#
# Replace the path here with the appropriate one for your machine#
myprojectpath = "/Users/eoinbrazil/Desktop/DublinR/TreesAndForests/DublinR-ML-treesandforests/"#
#
# Set the working directory to the current location for the project files#
setwd(myprojectpath)#
#
# Replace the path here with the appropriate one for your machine#
scriptpath = paste(myprojectpath, "scripts/", sep="")#
datapath = paste(myprojectpath, "data/", sep="")#
graphpath = paste(myprojectpath, "graphs/", sep="")#
# Set the seed so comparisons can be later made between the methods#
set.seed(2323)#
#
# Load Affairs data#
data(Affairs)#
#
# Explore the first ten records of the dataset to take a peek#
head(Affairs)#
#
# Look to get a summary of the dataset for each variable in the dataframe#
summary(Affairs)#
#
# Add some additional fields to data to for the correlation analysis#
Affairs$male <- mapvalues(Affairs$gender, from = c("female", "male"), to = c(0, 1))#
Affairs$male <- as.integer(Affairs$male)#
Affairs$male <- sapply(Affairs$male, function(x) x-1)#
Affairs$kids <- mapvalues(Affairs$children, from = c("no", "yes"), to = c(0, 1))#
Affairs$kids <- as.integer(Affairs$kids)#
Affairs$kids <- sapply(Affairs$kids, function(x) x-1)#
#
# Add a field to indicate if there was any affairs#
Affairs$hadaffair[Affairs$affairs< 1] <- 'No'#
Affairs$hadaffair[Affairs$affairs>=1] <- 'Yes'#
Affairs$hadaffair <- as.factor(Affairs$hadaffair)#
table(Affairs$hadaffair)#
#
# Check that the fields are only numerical in the data frame#
sapply(Affairs[, -c(2,5,12)], is.numeric)#
#
# Explore the data set to identify and then we'll remove any moderately correlated variables (could change the correlation value from 0.65 to .9)#
affairs.corr <- cor(Affairs[, -c(2,5,12])#
corrplot(affairs.corr, method = "number", tl.cex = 0.6)#
affairs.variables.corr <- findCorrelation(affairs.corr, 0.65)#
affairs.variables.corr.names <- colnames(affairs.corr[,affairs.variables.corr])#
#
# The dataset requires defined training and test subsets so let's remove some of the variables that don't see to add to the value and create these#
# in processing unwanted we also include the variables quality and color (13,15) as we want to remove these as well#
trainIndices = createDataPartition(Affairs$hadaffair, p = 0.8, list = F)#
#
unwanted = colnames(Affairs) %in% c("yearsmarried", "affairs")#
affairs.df.train = Affairs[trainIndices, !unwanted] #remove affairs and yearsmarried#
affairs.df.test = Affairs[!1:nrow(Affairs) %in% trainIndices, !unwanted]#
table(affairs.df.test$hadaffair)
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train,as.factor(hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train,as.factor(affairs.df.train$hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
imp <- rrf$importance
imp <- imp[,"MeanDecreaseGini"]#
subsetRRF <- which(imp>0) # produce the indices of the features selected by RRF
subsetRRF
imp
rrf
rf <- RRF(affairs.df.train,as.factor(affairs.df.train$hadaffair), flagReg = 0) # build an ordinary RF #
impRF <- rf$importance #
impRF <- impRF[,"MeanDecreaseGini"] # get the importance score #
imp <- impRF/(max(impRF)) #normalize the importance scores into [0,1]#
gamma <- 0.5   #A larger gamma often leads to fewer features. But, the quality of the features selected is quite stable for GRRF, i.e., different gammas can have similar accuracy performance (the accuracy of an ordinary RF using the feature subsets). See the paper for details. #
coefReg <- (1-gamma) + gamma*imp   # each variable has a coefficient, which depends on the importance score from the ordinary RF and the parameter: gamma#
grrf <- RRF(affairs.df.train,as.factor(affairs.df.train$hadaffair), flagReg=1, coefReg=coefReg)#
imp <- grrf$importance#
imp <- imp[,"MeanDecreaseGini"]#
subsetGRRF <- which(imp>0) # produce the indices of the features selected by GRRF#
#
print(subsetRRF) #the subset includes many more noisy variables than GRRF#
print(subsetGRRF)
colname(affairs.df.train)
colnames(affairs.df.train)
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower. #
#
imp <- rrf$importance#
imp <- imp[,"MeanDecreaseGini"]#
subsetRRF <- which(imp>0) # produce the indices of the features selected by RRF
subsetRRF
imp
rrf
Feature selection via GRRF#
rf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair), flagReg = 0) # build an ordinary RF #
impRF <- rf$importance #
impRF <- impRF[,"MeanDecreaseGini"] # get the importance score #
imp <- impRF/(max(impRF)) #normalize the importance scores into [0,1]#
gamma <- 0.5   #A larger gamma often leads to fewer features. But, the quality of the features selected is quite stable for GRRF, i.e., different gammas can have similar accuracy performance (the accuracy of an ordinary RF using the feature subsets). See the paper for details. #
coefReg <- (1-gamma) + gamma*imp   # each variable has a coefficient, which depends on the importance score from the ordinary RF and the parameter: gamma#
grrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair), flagReg=1, coefReg=coefReg)#
imp <- grrf$importance#
imp <- imp[,"MeanDecreaseGini"]#
subsetGRRF <- which(imp>0) # produce the indices of the features selected by GRRF#
#
print(subsetRRF) #the subset includes many more noisy variables than GRRF#
print(subsetGRRF)
subsetRRF
rf
grrf
results.rf
results.rf$FinalModel
results.rf$finalmodel
results.rf[1]
results.rf[1][1]
confusionMatrix(rfPred, affairs.df.test$hadaffair, positive="Yes")
rf
rfPrediction <- predict(rf, affairs.df.test[,-10])
rfPrediction
confusionMatrix(rfPrediction, affairs.df.test$hadaffair, positive="Yes")
confusionMatrix(rfPred, affairs.df.test$hadaffair, positive="Yes")
grrfPrediction <- predict(grrf, affairs.df.test[,-10])#
confusionMatrix(grrfPrediction, affairs.df.test$hadaffair, positive="Yes")
rf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair), flagReg = 0) # build an ordinary RF #
impRF <- rf$importance #
impRF <- impRF[,"MeanDecreaseGini"] # get the importance score #
imp <- impRF/(max(impRF)) #normalize the importance scores into [0,1]#
gamma <- 0.4   #A larger gamma often leads to fewer features. But, the quality of the features selected is quite stable for GRRF, i.e., different gammas can have similar accuracy performance (the accuracy of an ordinary RF using the feature subsets). See the paper for details. #
coefReg <- (1-gamma) + gamma*imp   # each variable has a coefficient, which depends on the importance score from the ordinary RF and the parameter: gamma#
grrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair), flagReg=1, coefReg=coefReg)#
imp <- grrf$importance#
imp <- imp[,"MeanDecreaseGini"]#
subsetGRRF <- which(imp>0) # produce the indices of the features selected by GRRF#
#
print(subsetRRF) #the subset includes many more noisy variables than GRRF#
print(subsetGRRF)#
#
rfPrediction <- predict(rf, affairs.df.test[,-10])#
confusionMatrix(rfPrediction, affairs.df.test$hadaffair, positive="Yes")#
grrfPrediction <- predict(grrf, affairs.df.test[,-10])#
confusionMatrix(grrfPrediction, affairs.df.test$hadaffair, positive="Yes")
gamma <- 0.8  #A larger gamma often leads to fewer features. But, the quality of the features selected is quite stable for GRRF, i.e., different gammas can have similar accuracy performance (the accuracy of an ordinary RF using the feature subsets). See the paper for details. #
coefReg <- (1-gamma) + gamma*imp   # each variable has a coefficient, which depends on the importance score from the ordinary RF and the parameter: gamma#
grrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair), flagReg=1, coefReg=coefReg)#
imp <- grrf$importance#
imp <- imp[,"MeanDecreaseGini"]#
subsetGRRF <- which(imp>0) # produce the indices of the features selected by GRRF#
#
print(subsetRRF) #the subset includes many more noisy variables than GRRF#
print(subsetGRRF)#
#
rfPrediction <- predict(rf, affairs.df.test[,-10])#
confusionMatrix(rfPrediction, affairs.df.test$hadaffair, positive="Yes")#
grrfPrediction <- predict(grrf, affairs.df.test[,-10])#
confusionMatrix(grrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.7 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.9 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=c(2:6),flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
rrf
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=4,flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrf
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=4,ntree=1000,flagReg=1,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
rrf
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=4,ntree=1000,flagReg=1,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrf
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=1,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrf
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=0,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=4,ntree=1000,flagReg=0,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),mtry=6,ntree=1000,flagReg=0,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=0,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=0,importance=FALSE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
rrfPrediction <- predict(rrf, affairs.df.test[,-10])#
confusionMatrix(rrfPrediction, affairs.df.test$hadaffair, positive="Yes")
lambda <- 0.8 # Both the number of features and the quality of the features are quite sensitive to lambda for RRF. A smaller lambda leads to fewer features.#
rrf <- RRF(affairs.df.train[,-10],as.factor(affairs.df.train$hadaffair),flagReg=0,importance=TRUE,coefReg=lambda) # coefReg is a constant for all variables.   #either "X,as.factor(class)" or data frame like "Y~., data=data" is fine, but the later one is significantly slower.
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=200)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.25, ntreeTry=200)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1, ntreeTry=100)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=100)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=200)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.75, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=2, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=25)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=50)
tunedRRF <- tuneRRF(affairs.df.test[,-10], affairs.df.test[,10], stepFactor=1.5, ntreeTry=50)
For data manipulation and visualization#
library(arules)#
library(arulesViz)#
library(ElemStatLearn)#
library(car)#
library(Rgraphviz)#
#
# Replace the path here with the appropriate one for your machine#
myprojectpath = "/Users/eoinbrazil/Desktop/DublinR/TreesAndForests/DublinR-ML-treesandforests/"#
#
# Set the working directory to the current location for the project files#
setwd(myprojectpath)#
#
# Replace the path here with the appropriate one for your machine#
scriptpath = paste(myprojectpath, "scripts/", sep="")#
datapath = paste(myprojectpath, "data/", sep="")#
graphpath = paste(myprojectpath, "graphs/", sep="")#
#
# Load and take a look at the data#
data(marketing)#
head(marketing)#
summary(marketing)#
#
# Change the data to ensure it can be used as only categorical#
marketing.income <- recode(marketing$Income,"1='<$10,000'; 2='$10,000 to $14,999'; 3='$15,000 to $19,999'; 4='$20,000 to $24,999'; 5='$25,000 to $29,999'; 6='$30,000 to $39,999'; 7='$40,000 to $49,999'; 8='$50,000 to $74,999'; 9='$75,000+'")#
marketing.sex <- recode(marketing$Sex,"1='Male'; 2='Female'")#
marketing.martial <- recode(marketing$Martial, "1='Married'; 2='Living together, not married'; 3='Divorced or separated'; 4='Windowed'; 5='Single, never married'; else='Unknown'")#
marketing.age <- recode(marketing$Age, "1='14-17'; 2='18-24'; 3='25-34'; 4='35-44'; 5='45-54'; 6='55-64'; 7='65+' ")#
marketing.edu <- recode(marketing$Edu, "1='Grade 8 or less'; 2='Grades 9 to 11'; 3='Graduated high school'; 4='1 to 3 years of college'; 5='College graduate'; 6='Postgraduate study'; else='Unknown'")#
marketing.occupation <- recode(marketing$Occupation, "1='Professional/Managerial'; 2='Sales Worker'; 3='Factory Worker/Laborer/Driver'; 4='Clerical/Service Worker'; 5='Homemaker'; 6='Student, HS or College'; 7='Military'; 8='Retired'; 9='Unemployed'; else='Unknown'")#
marketing.lived <- recode(marketing$Lived, "1='Less than a year'; 2='One to three years'; 3='Four to six years'; 4='Seven to ten years'; 5='More than ten year'; else='Unknown'")#
marketing.dualincome <- recode(marketing$Dual_Income, "1='Not married'; 2='Yes'; 3='No'")#
marketing.householdsize <- recode(marketing$Household, "1='1'; 2='2'; 3='3'; 4='4'; 5='5'; 6='6'; 7='7'; 8='8'; 9='9'; else='Unknown'")#
marketing.householdsizeunder18years <- recode(marketing$Householdu18, "1='1'; 2='2'; 3='3'; 4='4'; 5='5'; 6='6'; 7='7'; 8='8'; 9='9'; else='Unknown'")#
marketing.status <- recode(marketing$Status, "1='Own'; 2='Rent'; 3='Live with Parents/Family'; else='Unknown'")#
marketing.hometype <- recode(marketing$Home_Type, "1='House'; 2='Condominium'; 3='Apartment'; 4='Mobile Home'; 5='Other'; else='Unknown'")#
marketing.ethnic <- recode(marketing$Ethnic, "1='American Indian'; 2='Asian'; 3='Black'; 4='East Indian'; 5='Hispanic'; 6='Pacific Islander'; 7='White'; 8='Other'; else='Unknown'")#
marketing.language <- recode(marketing$Language, "1='English'; 2='Spanish'; 3='Other'; else='Unknown'")#
#
# Create a new dataframe and removed the earlier variables to clean up space#
marketing.df <- data.frame(marketing.income, marketing.sex, marketing.martial, marketing.age, marketing.edu, marketing.occupation, marketing.lived, marketing.dualincome, marketing.householdsize, marketing.householdsizeunder18years, marketing.status, marketing.hometype, marketing.ethnic, marketing.language)#
rm(marketing.income, marketing.sex, marketing.martial, marketing.age, marketing.edu, marketing.occupation, marketing.lived, marketing.dualincome, marketing.householdsize, marketing.householdsizeunder18years, marketing.status, marketing.hometype, marketing.ethnic, marketing.language)#
#
# Transform the data frame to a transactions object#
marketing.transactions <- as(marketing.df, "transactions")
library(car)
For data manipulation and visualization#
library(arules)#
library(arulesViz)#
library(ElemStatLearn)#
library(car)#
library(Rgraphviz)#
#
# Replace the path here with the appropriate one for your machine#
myprojectpath = "/Users/eoinbrazil/Desktop/DublinR/TreesAndForests/DublinR-ML-treesandforests/"#
#
# Set the working directory to the current location for the project files#
setwd(myprojectpath)#
#
# Replace the path here with the appropriate one for your machine#
scriptpath = paste(myprojectpath, "scripts/", sep="")#
datapath = paste(myprojectpath, "data/", sep="")#
graphpath = paste(myprojectpath, "graphs/", sep="")#
#
# Load and take a look at the data#
data(marketing)#
head(marketing)#
summary(marketing)#
#
# Change the data to ensure it can be used as only categorical#
marketing.income <- recode(marketing$Income,"1='<$10,000'; 2='$10,000 to $14,999'; 3='$15,000 to $19,999'; 4='$20,000 to $24,999'; 5='$25,000 to $29,999'; 6='$30,000 to $39,999'; 7='$40,000 to $49,999'; 8='$50,000 to $74,999'; 9='$75,000+'")#
marketing.sex <- recode(marketing$Sex,"1='Male'; 2='Female'")#
marketing.martial <- recode(marketing$Martial, "1='Married'; 2='Living together, not married'; 3='Divorced or separated'; 4='Windowed'; 5='Single, never married'; else='Unknown'")#
marketing.age <- recode(marketing$Age, "1='14-17'; 2='18-24'; 3='25-34'; 4='35-44'; 5='45-54'; 6='55-64'; 7='65+' ")#
marketing.edu <- recode(marketing$Edu, "1='Grade 8 or less'; 2='Grades 9 to 11'; 3='Graduated high school'; 4='1 to 3 years of college'; 5='College graduate'; 6='Postgraduate study'; else='Unknown'")#
marketing.occupation <- recode(marketing$Occupation, "1='Professional/Managerial'; 2='Sales Worker'; 3='Factory Worker/Laborer/Driver'; 4='Clerical/Service Worker'; 5='Homemaker'; 6='Student, HS or College'; 7='Military'; 8='Retired'; 9='Unemployed'; else='Unknown'")#
marketing.lived <- recode(marketing$Lived, "1='Less than a year'; 2='One to three years'; 3='Four to six years'; 4='Seven to ten years'; 5='More than ten year'; else='Unknown'")#
marketing.dualincome <- recode(marketing$Dual_Income, "1='Not married'; 2='Yes'; 3='No'")#
marketing.householdsize <- recode(marketing$Household, "1='1'; 2='2'; 3='3'; 4='4'; 5='5'; 6='6'; 7='7'; 8='8'; 9='9'; else='Unknown'")#
marketing.householdsizeunder18years <- recode(marketing$Householdu18, "1='1'; 2='2'; 3='3'; 4='4'; 5='5'; 6='6'; 7='7'; 8='8'; 9='9'; else='Unknown'")#
marketing.status <- recode(marketing$Status, "1='Own'; 2='Rent'; 3='Live with Parents/Family'; else='Unknown'")#
marketing.hometype <- recode(marketing$Home_Type, "1='House'; 2='Condominium'; 3='Apartment'; 4='Mobile Home'; 5='Other'; else='Unknown'")#
marketing.ethnic <- recode(marketing$Ethnic, "1='American Indian'; 2='Asian'; 3='Black'; 4='East Indian'; 5='Hispanic'; 6='Pacific Islander'; 7='White'; 8='Other'; else='Unknown'")#
marketing.language <- recode(marketing$Language, "1='English'; 2='Spanish'; 3='Other'; else='Unknown'")#
#
# Create a new dataframe and removed the earlier variables to clean up space#
marketing.df <- data.frame(marketing.income, marketing.sex, marketing.martial, marketing.age, marketing.edu, marketing.occupation, marketing.lived, marketing.dualincome, marketing.householdsize, marketing.householdsizeunder18years, marketing.status, marketing.hometype, marketing.ethnic, marketing.language)#
rm(marketing.income, marketing.sex, marketing.martial, marketing.age, marketing.edu, marketing.occupation, marketing.lived, marketing.dualincome, marketing.householdsize, marketing.householdsizeunder18years, marketing.status, marketing.hometype, marketing.ethnic, marketing.language)#
#
# Transform the data frame to a transactions object#
marketing.transactions <- as(marketing.df, "transactions")
marketing.income <- recode(marketing$Income,"1='<$10,000'; 2='$10,000 to $14,999'; 3='$15,000 to $19,999'; 4='$20,000 to $24,999'; 5='$25,000 to $29,999'; 6='$30,000 to $39,999'; 7='$40,000 to $49,999'; 8='$50,000 to $74,999'; 9='$75,000+'")
library(car)
marketing.income <- recode(marketing$Income,"1='<$10,000'; 2='$10,000 to $14,999'; 3='$15,000 to $19,999'; 4='$20,000 to $24,999'; 5='$25,000 to $29,999'; 6='$30,000 to $39,999'; 7='$40,000 to $49,999'; 8='$50,000 to $74,999'; 9='$75,000+'")
