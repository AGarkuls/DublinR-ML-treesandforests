save.pkg.list <- installed.packages()[is.na(installed.packages()[ , "Priority"]), 1]
save(save.pkg.list, file="pkglist.Rdata")
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list
typeof(save.pkg.list)
x<-x[-which(x==4)]
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="zoo")]
save.pkg.list
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
save.pkg.list
load("pkglist.Rdata")#
install.packages(save.pkg.list)
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
install.packages(save.pkg.list)
Sys.getenv("PATH")
load("pkglist.Rdata")
save.pkg.list<-save.pkg.list[-which(save.pkg.list=="financial")]
install.packages(save.pkg.list)
library(party)#
library(rpart)#
library(ggplot2)#
library(randomForest)
head(kyphosis)
summary(kyphosis)
length(kyphosis)
require(devtools)
install_github("slidify", "ramnathv")
library(corrplot)#
library(caret)#
# Replace the path here with the appropriate one for your machine#
myprojectpath = "/Users/eoinbrazil/Desktop/DublinR/TreesAndForests/DublinR-ML-treesandforests/"#
#
# Set the working directory to the current location for the project files#
setwd(myprojectpath)#
#
# Replace the path here with the appropriate one for your machine#
scriptpath = paste(myprojectpath, "scripts/", sep="")#
datapath = paste(myprojectpath, "data/", sep="")#
graphpath = paste(myprojectpath, "graphs/", sep="")#
#
# Load wine dataset into a dataframe for processing#
wine.df = read.csv(paste(datapath, "goodwine.csv", sep=""))#
#
# Explore the first ten records of the dataset to take a peek#
head(wine.df)#
#
# Look to get a summary of the dataset for each variable in the dataframe#
summary(wine.df)#
# Plot the correlation excluding color and goodness parameters#
corrplot(cor(wine.df[, -c(13, 15)]), method = "number", tl.cex = 0.5)
trainIndices = createDataPartition(wine.df$good, p = 0.8, list = F)
unwanted = colnames(wine.df) %in% c("free.sulfur.dioxide", "density", "quality",#
"color", "white")
unwanted
wine.df.train = wine.df[trainIndices, !unwanted] #remove quality and color, as well as density and others#
wine.df.test = wine.df[!1:nrow(wine.df) %in% trainIndices, !unwanted]
wine.df.trainplot = predict(preProcess(wine.df.train[,-10], method="range"), wine.df.train[,-10])#
featurePlot(wine.df.trainplot, wine.df.train$good, "box")
library(randomForest)
cv.opts = trainControl(method="cv", number=10)#
rf.opts = data.frame(.mtry=c(2:6))#
results.rf = train(good~., data=wine.df.train, method="rf", preProcess='range',trControl=cv.opts, tuneGrid=rf.opts, n.tree=1000)
library(rpart.plot)	# Fancy tree plot#
library(rattle) # Nice tree plot from rattle - fancyRpartPlot#
library(RColorBrewer)
results_rf
results.rf
plot(results.rf, scales = list(x = list(log = 10)))
rfPred <- predict(results.rf, wine.df.test)
confusionMatrix(rfPred, wine.df.test$good)
rfProbs <- predict(rfPred, wine.df.test, type = "prob")
prp(results.rf$finalModel)				# Will plot the tree
fancyRpartPlot(results.rf$finalModel)
getTree(results.rf, 1, labelVar=TRUE)
results.rf
class(results.rf$finalModel)
varImpPlot(results.rf)
rfPred <- predict(results.rf, wine.df.test[,-10])#
confusionMatrix(rfPred, wine.df.test$good, positive='Good')
rfProbs <- predict(rfPred, wine.df.test, type = "prob")
rfPredProb <- predict(results.rf, wine.df.test[,-10], type='prob')
rfROC <- roc(wine.df.test$good, rfPredProb[, "PS"], levels = rev(wine.df.test$good))
library(pROC)
rfROC <- roc(wine.df.test$good, rfPredProb[, "PS"], levels = rev(wine.df.test$good))
rfPredProb
head(rfPredProb)
plot(rfPredProb)
randomForest.plot(rfPredProb)
rfROC <- roc(wine.df.test$good, rfPredProb[, "Good"], levels = rev(wine.df.test$good))
plot(rfROC)
rfROC
plot(rfROC, type = "S", print.thres = .5)
prp(rfPredProb)
plot(rfROC, type = "S", print.thres = .5)
confusionMatrix(rfPred, wine.df.test$good, positive='Good')
confusionMatrix(rfPred, wine.df.test$good)
confusionMatrix(rfPred, wine.df.test$good, positive='Good')
library(AppliedPredictiveModeling)
data(segmentationData)
Load Image Segmentation data#
data(segmentationData)#
#
# Remove the Cell identification data#
segmentationData$Cell <- NULL#
#
# The dataset already has defined training and test subsets, no need to subset#
training <- subset(segmentationData, Case == "Train")#
testing <- subset(segmentationData, Case == "Test")#
# Remove the Case identification data#
training$Case <- NULL#
testing$Case <- NULL#
str(training[,1:6])#
#
# Use three repeats of 10–fold cross–validation to train the tree#
# 10-fold CV can be noisy/over-fitted for small to moderate sample sizes but we'll risk it for higher performances#
# The CART algorithm uses overall accuracy and the one standard–error rule to prune the tree, however we can also choose the tree complexity based on the largest absolute area under the ROC curve.#
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3, summaryFunction = twoClassSummary, classProbs = TRUE)#
rpartTune <- train(Class ~ ., data = training, method = "rpart", tuneLength = 30, metric = "ROC", trControl = cvCtrl)#
#
# Plot the#
plot(rpartTune, scales = list(x = list(log = 10)))#
#
# Look at the results of the tuned tree based on the testing dataset for predicting new samples#
rpartPred <- predict(rpartTune, testing)
head(testing$Class)
confusionMatrix(rpartPred, testing$Class, positive='WS')
confusionMatrix(rfPred, wine.df.test$good, positive='Good')
339 / (339 + 72 + 137 + 750)
750 / (339 + 72 + 137 + 750)
137 / (339 + 72 + 137 + 750)
72 / (339 + 72 + 137 + 750)
26+58+6+11
cor(wine[, -c(13, 15)]
)
cor(wine[, -c(13, 15)],)
cor(wine[, -c(13, 15)])
wine
cor(wine)
cor(wine[, -c(13, 15)],)
wine[, -c(13, 15)]
head(wine[, -c(13, 15)])
is.numeric(head(wine[, -c(13, 15)]))
as.matrix(head(wine[, -c(13, 15)]))
cor(as.matrix(head(wine[, -c(13, 15)])))
cor(wine[, -c(13, 15)],)
library(corrgram)
library(corrgram)
library(corrgram)
corrplot(cor(wine.df[, -c(13, 15)]), method = "number", tl.cex = 0.5)
corrgam(cor(wine.df[, -c(13, 15)]), method = "number", tl.cex = 0.5)
corrgram(cor(wine.df[, -c(13, 15)]), method = "number", tl.cex = 0.5)
corrgram(cor(wine.df[, -c(13, 15)]),  order=NULL, lower.panel=panel.shade,#
  upper.panel=NULL, text.panel=panel.txt,#
  main="Wine Data (unsorted)")
wine
wine.df
cor(wine.df[, -c(13, 15)],)
wine.df.train
length(wine.df.train)
count(wine.df.train)
length(wine.df.train[,1])
length(wine.df.test[,1])
cv.opts = trainControl(method="cv", number=10)#
knn.opts = data.frame(.k=c(seq(3, 11, 2), 25, 51, 101)) #odd to avoid ties#
results.knn = train(good~., data=wine.df.train, method="knn", preProcess="range", trControl=cv.opts, tuneGrid = knn.opts)
plot(results.knn)
results.knn
knnPred <- predict(results.knn, wine.df.test[,-10])
confusionMatrix(knnPred, wine.df.test$good, positive='Good')
knnPredProb <- predict(results.knn, wine.df.test[,-10], type='prob')#
knnROC <- roc(wine.df.test$good, knnPredProb[, "Good"], levels = rev(wine.df.test$good))#
plot(knnROC, type = "S", print.thres = .5)
cv.opts = trainControl(method="cv", number=10)#
results.nnet = train(good~., data=wine.df.train, method="avNNet", trControl=cv_opts, preProcess="range",#
tuneLength=5, trace=F, maxit=1000)#
#
# Plot the#
plot(results.nnet)#
results_nnet
cv.opts = trainControl(method="cv", number=10)#
results.nnet = train(good~., data=wine.df.train, method="avNNet", trControl=cv.opts, preProcess="range",#
tuneLength=5, trace=F, maxit=1000)#
#
# Plot the#
plot(results.nnet)#
results_nnet
results.nnet = train(good~., data=wine.df.train, method="avNNet", trControl=cv.opts, preProcess="range",#
tuneLength=3, trace=F, maxit=1000)
